# @package _global_

scratch:
  resolution: 1024 # 학습 시 사용할 이미지 해상도 (기본: 1024x1024)
  train_batch_size: 1 # 학습 배치 크기
  num_train_workers: 10 # 데이터 로딩에 사용하는 CPU 스레드 수 (코어 수의 80%정도 권장)
  num_frames: 8 # 학습 시 한 iteration에서 훑어볼 비디오 프레임 수 (메모리 어텐션을 학습시키기 위한 비디오 배치) (이미지: 1)
  max_num_objects: 3 # 한 장면에서 동시에 다룰 개별 물체의 최대 개수 (이 이미지 안에는 객체가 최대 몇 개 있는가)
  base_lr: 5.0e-6 # 이미지 인코더 외 모든 모듈에 적용되는 학습률 (높게)
  vision_lr: 3.0e-06 # 이미지 인코더에 적용되는 학습률 (낮게)
  phases_per_epoch: 1 # 한 epoch당 저장할 체크포인트 개수/학습률 조정 횟수/검증 횟수/로그 횟수 등
  num_epochs: 40 # 총 학습 epoch 수

dataset: # 추가 코드 해석 필요
  img_folder: null # PATH to MOSE JPEGImages folder
  gt_folder: null  # PATH to MOSE Annotations folder
  file_list_txt: training/assets/MOSE_sample_train_list.txt # Optional PATH to filelist containing a subset of videos to be used for training
  multiplier: 2 # 한 에폭 동안 동일한 데이터셋을 몇 번 반복(Agmentation)하여 사용할지 (Agmentation 사용X: 1)

# Video transforms
vos:
  train_transforms:
    - _target_: training.dataset.transforms.ComposeAPI # 변형 기법의 조합 명령어
      transforms:
        - _target_: training.dataset.transforms.RandomHorizontalFlip # 수평 뒤집기(무작위)  # p 필요한지 확인 필요
          consistent_transform: True # 프레임 간 일관된 변형 적용
        - _target_: training.dataset.transforms.RandomAffine # 무작위 회전/비틀기(shear)
          degrees: 25 # 회전 각도 (기본: 25도)
          shear: 20 # 비틀기 정도 (기본: 20도)
          image_interpolation: bilinear # 회전/비틀기 시 보간법 (기본: bilinear)
          consistent_transform: True
        - _target_: training.dataset.transforms.RandomResizeAPI # 크기 조정
          sizes: ${scratch.resolution} # scratch.resolution에 정의된 해상도로 크기 조정
          square: true
          consistent_transform: True
        - _target_: training.dataset.transforms.ColorJitter  # 밝기, 대비, 채도 조정
          consistent_transform: True
          brightness: 0.1 # 밝기 변화 범위 (원본의 최대 10% 범위에서 무작위 조정) (기본: 0.1)
          contrast: 0.03 # 대비 변화 범위 (기본: 0.03)
          saturation: 0.03 # 채도 변화 범위 (기본: 0.03)
          hue: null # 색상 변화 범위 (기본: null, 변화 없음)
        - _target_: training.dataset.transforms.RandomGrayscale # 이미지를 흑백으로 변경
          p: 0.05 # 확률
          consistent_transform: True
        - _target_: training.dataset.transforms.ColorJitter # 밝기, 대비, 채도 조정 (두 번째 적용)
          consistent_transform: False # 프레임 간 일관된 변형 적용 하지 않음
          brightness: 0.1
          contrast: 0.05
          saturation: 0.05
          hue: null
        - _target_: training.dataset.transforms.ToTensorAPI # 이미지를 텐서로 변환
        - _target_: training.dataset.transforms.NormalizeAPI # 정규화
          mean: [0.485, 0.456, 0.406] # ImageNet 평균값
          std: [0.229, 0.224, 0.225] # ImageNet 표준편차값

trainer:
  _target_: training.trainer.Trainer # Trainer 클래스의 위치 (training/trainer.py -> Trainer 클래스)
  mode: train_only # 학습 모드 설정 (train: 학습 후 검증 수행, train_only: 학습만 수행, val: 검증만 수행)
  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}} # 총 phase(저장할 체크포인트)의 개수 (산정식 활용)
  accelerator: cuda # 가속기 설정 (cuda: GPU 사용, cpu: CPU 사용)
  seed_value: 123 # 랜덤 시드 값 설정 (동일한 코드에서는 동일한 실험 결과가 나옴)

  model:
    _target_: training.model.sam2.SAM2Train # 학습용 SAM2 구현체의 클래스 위치 (training/model/sam2.py -> SAM2Train 클래스)
    image_encoder:
      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder # 이미지 인코더 클래스 위치 (sam2/modeling/backbones/image_encoder.py -> ImageEncoder 클래스)
      scalp: 1 # Scalable Pyramid 계층 수 (기본: 1, 유지)
      trunk:
        _target_: sam2.modeling.backbones.hieradet.Hiera
        embed_dim: 112 # 이미지 임베딩 차원 (Large: 144, Base+: 112, Small: 96, Tiny: 96)
        num_heads: 2 # Attention 헤드 수 (Large: 2, Base+: 2, Small: 3, Tiny: 3)
        drop_path_rate: 0.1 # 트랜스포머 블록의 잔차 경로(Residual Path) 전체를 통째로 10% 무작위로 생략 (오버피팅 방지) (기본: 0.1)
      neck:
        _target_: sam2.modeling.backbones.image_encoder.FpnNeck # 해상도별 특징 결합 (FpnNeck)
        position_encoding:
          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine # 위치 인코딩 방식(기본: Sine)
          num_pos_feats: 256 # 위치 인코딩 개수, d_model과 동일 (기본: 256, 유지)
          normalize: true # 정규화 적용 여부 (기본: true, 유지)
          scale: null
          temperature: 10000 # 사인 곡선 주기 조절 (기본: 10000, 유지, 클수록 전체적인 공간적 흐름을 더 잘 잡을 수 있음)
        d_model: 256 # 특징 맵의 채널 수 (기본: 256, 유지)
        backbone_channel_list: [896, 448, 224, 112]
        # Large: [1152, 576, 288, 144]
        # Base+: [896, 448, 224, 112]
        # Small: [768, 384, 192, 96]
        # Tiny: [768, 384, 192, 96]
        fpn_top_down_levels: [2, 3]  # output level 0 and 1 directly use the backbone features
        fpn_interp_model: nearest

    memory_attention: # 이미지 FineTuning에서는 사용하지 않으므로 유지
      _target_: sam2.modeling.memory_attention.MemoryAttention
      d_model: 256
      pos_enc_at_input: true
      layer:
        _target_: sam2.modeling.memory_attention.MemoryAttentionLayer
        activation: relu
        dim_feedforward: 2048
        dropout: 0.1
        pos_enc_at_attn: false
        self_attention:
          _target_: sam2.modeling.sam.transformer.RoPEAttention
          rope_theta: 10000.0
          feat_sizes: [64, 64]
          embedding_dim: 256
          num_heads: 1
          downsample_rate: 1
          dropout: 0.1
        d_model: 256
        pos_enc_at_cross_attn_keys: true
        pos_enc_at_cross_attn_queries: false
        cross_attention:
          _target_: sam2.modeling.sam.transformer.RoPEAttention
          rope_theta: 10000.0
          feat_sizes: [64, 64]
          rope_k_repeat: True
          embedding_dim: 256
          num_heads: 1
          downsample_rate: 1
          dropout: 0.1
          kv_in_dim: 64
      num_layers: 4

    memory_encoder: # 이미지 FineTuning에서는 사용하지 않으므로 유지
        _target_: sam2.modeling.memory_encoder.MemoryEncoder
        out_dim: 64
        position_encoding:
          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine
          num_pos_feats: 64
          normalize: true
          scale: null
          temperature: 10000
        mask_downsampler:
          _target_: sam2.modeling.memory_encoder.MaskDownSampler
          kernel_size: 3
          stride: 2
          padding: 1
        fuser:
          _target_: sam2.modeling.memory_encoder.Fuser
          layer:
            _target_: sam2.modeling.memory_encoder.CXBlock
            dim: 256
            kernel_size: 7
            padding: 3
            layer_scale_init_value: 1e-6
            use_dwconv: True  # depth-wise convs
          num_layers: 2

    ############################################################
    # 확인 1, 2, 3 제외 옵션들은 전부 비디오 관련
    ############################################################
    num_maskmem: 7 
    image_size: ${scratch.resolution}
    # apply scaled sigmoid on mask logits for memory encoder, and directly feed input mask as output mask
    sigmoid_scale_for_mem_enc: 20.0
    sigmoid_bias_for_mem_enc: -10.0
    use_mask_input_as_output_without_sam: true
    # Memory
    directly_add_no_mem_embed: true
    no_obj_embed_spatial: true
    # use high-resolution feature map in the SAM mask decoder
    use_high_res_features_in_sam: true # [확인 1] Skip Connection 사용 (기본값: true)
    # output 3 masks on the first click on initial conditioning frames
    multimask_output_in_sam: true # [확인 2] Multimask Output 사용 (기본값: true)
    # SAM heads
    iou_prediction_use_sigmoid: True # [확인 3] IoU 예측 시 시그모이드 함수 사용 (기본값: True)
    # cross-attend to object pointers from other frames (based on SAM output tokens) in the encoder
    use_obj_ptrs_in_encoder: true
    add_tpos_enc_to_obj_ptrs: true
    proj_tpos_enc_in_obj_ptrs: true
    use_signed_tpos_enc_to_obj_ptrs: true
    only_obj_ptrs_in_the_past_for_eval: true
    # object occlusion prediction
    pred_obj_scores: true
    pred_obj_scores_mlp: true
    fixed_no_obj_ptr: true
    # multimask tracking settings
    multimask_output_for_tracking: true
    use_multimask_token_for_obj_ptr: true
    multimask_min_pt_num: 0
    multimask_max_pt_num: 1
    use_mlp_for_obj_ptr_proj: true
    # Compilation flag
    # compile_image_encoder: False
    ############################################################

    ################# Training specific params #################
    # 이미지 1장 기준 학습 시 point prompt 1개만 사용되도록 하드코딩 되어 있음
    # 아래 옵션들은 비디오 관련 옵션
    ############################################################
    # box/point input and corrections
    prob_to_use_pt_input_for_train: 0.5
    prob_to_use_pt_input_for_eval: 0.0
    prob_to_use_box_input_for_train: 0.5
    prob_to_use_box_input_for_eval: 0.0
    prob_to_sample_from_gt_for_train: 0.1  # with a small prob, sampling correction points from GT mask instead of prediction errors
    num_frames_to_correct_for_train: 2  # iteratively sample on random 1~2 frames (always include the first frame)
    num_frames_to_correct_for_eval: 1  # only iteratively sample on first frame
    rand_frames_to_correct_for_train: True  # random #init-cond-frame ~ 2
    add_all_frames_to_correct_as_cond: True  # when a frame receives a correction click, it becomes a conditioning frame (even if it's not initially a conditioning frame)
    ############################################################
    # maximum 2 initial conditioning frames
    num_init_cond_frames_for_train: 2 # 기준 프레임 개수 (비디오)
    rand_init_cond_frames_for_train: True  # 기준 프레임 개수 무작위화 여부 (비디오)
    num_correction_pt_per_frame: 7 # 프레임 1개(이미지) 당 최대 교정 포인트 수
    use_act_ckpt_iterative_pt_sampling: false # Activation Checkpointing (메모리 절약-계산 속도, 현재는 속도 선택)
    

    
    num_init_cond_frames_for_eval: 1  # 평가 시 기준 프레임 개수 (비디오)
    forward_backbone_per_frame_for_eval: True # 평가 시 프레임별로 백본 순전파 수행 여부 (비디오)
    ############################################################
    

  data: # 데이터 가공 관련 옵션
    train:
      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset # 학습 시 사용할 데이터로더 (TorchTrainMixedDataset)
      phases_per_epoch: ${scratch.phases_per_epoch} # 위에서 정의한 에폭당 phase 수
      batch_sizes:
        - ${scratch.train_batch_size} # 위에서 정의한 배치 크기

      datasets: # 사용할 데이터셋 종류
        - _target_: training.dataset.utils.RepeatFactorWrapper
          dataset:
            _target_: training.dataset.utils.ConcatDataset # 여러 데이터셋을 하나로 합침 (현재는 1개 데이터셋만 사용)
            datasets:
            - _target_: training.dataset.vos_dataset.VOSDataset # 기존: VOSDataset
              transforms: ${vos.train_transforms}
              training: true
              video_dataset:
                _target_: training.dataset.vos_raw_dataset.PNGRawDataset # 원본 이미지 데이터를 불러오는 데이터로더 (사용자 정의 데이터로더로 변경)
                img_folder: ${dataset.img_folder} # 위에서 정의한 이미지 폴더 경로
                gt_folder: ${dataset.gt_folder} # 위에서 정의한 GT 폴더 경로
                file_list_txt: ${dataset.file_list_txt} # 위에서 정의한 파일 리스트 경로
              sampler:
                _target_: training.dataset.vos_sampler.RandomUniformSampler # 데이터셋에서 프레임 샘플링 방식 (비디오에서만)
                num_frames: ${scratch.num_frames} # 위에서 정의한 프레임 수 (이미지: 1)
                max_num_objects: ${scratch.max_num_objects} # 위에서 정의한 최대 객체 수
              multiplier: ${dataset.multiplier} # 데이터셋 반복 횟수 (Agmentation)
      shuffle: True # 데이터셋 셔플 여부 (항상 True)
      num_workers: ${scratch.num_train_workers} # 데이터 로딩에 사용할 CPU 스레드 수
      pin_memory: True # 데이터 로딩 시 핀 메모리 사용 여부 (기본: True)
      drop_last: True # 마지막 미니배치 버림 여부 (기본: True)
      collate_fn: # 낱개 이미지들을 결합하여 batch 단위로 묶는 함수
        _target_: training.utils.data_utils.collate_fn 
        _partial_: true
        dict_key: all 

  optim: # 옵티마이저 관련 설정
    amp: # 자동 혼합 정밀도(Automatic Mixed Precision) 설정
      enabled: True
      amp_dtype: bfloat16

    optimizer: # 가중치 업데이트
      _target_: torch.optim.AdamW # AdamW 옵티마이저 사용

    gradient_clip: # 그래디언트 클리핑 설정
      _target_: training.optimizer.GradientClipper
      max_norm: 0.1 # 최대 기울기(기본: 0.1)
      norm_type: 2

    param_group_modifiers: # 부위별 차등 학습 (입력에 가까운 층일수록 낮은 학습률 적용)
      - _target_: training.optimizer.layer_decay_param_modifier
        _partial_: True
        layer_decay_value: 0.9 # 층별 학습률 감쇠 비율 (기본: 0.9)
        apply_to: 'image_encoder.trunk' # 감쇠 적용 대상 (이미지 인코더의 트렁크)
        overrides: # 감쇠 적용 제외 대상 (위치 임베딩은 확실하게 학습)
          - pattern: '*pos_embed*'
            value: 1.0

    options:
      lr: # 학습률
        - scheduler: # 이미지 인코더 외 모든 모듈에 적용
            _target_: fvcore.common.param_scheduler.CosineParamScheduler # 코사인 스케줄러 사용
            start_value: ${scratch.base_lr} # 사전 정의된 시작 학습률
            end_value: ${divide:${scratch.base_lr},10} # 시작 학습률의 1/10을 종료 학습률로 설정
        - scheduler:
            _target_: fvcore.common.param_scheduler.CosineParamScheduler # 코사인 스케줄러 사용
            start_value: ${scratch.vision_lr} # 사전 정의된 이미지 인코더 학습률
            end_value: ${divide:${scratch.vision_lr},10} # 이미지 인코더 학습률의 1/10을 종료 학습률로 설정
          param_names:
            - 'image_encoder.*' # 이미지 인코더에 대해서만 적용
      weight_decay: # 가중치 감쇠
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler # 일정한 가중치 감쇠 사용
            value: 0.1 # 가중치 감쇠 값
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler # 일정한 가중치 감쇠 사용
            value: 0.0 # 가중치 감쇠 없음
          param_names:
            - '*bias*' # bias 파라미터에 대해서는 가중치 감쇠 없음 (데이터 중심선)
          module_cls_names: ['torch.nn.LayerNorm'] # 정규화 Layer에 대해서는 가중치 감쇠 없음 (학습 안정성)

  loss:
    all:
      _target_: training.loss_fns.MultiStepMultiMasksAndIous # 다중 단계 다중 마스크 및 IoU 손실 함수 (사전 정의된 sam2의 loss 사용)
      weight_dict:
        loss_mask: 20 # 마스크 손실 가중치(Focal Loss: 20)
        loss_dice: 1 # Dice 손실 가중치 (1)
        loss_iou: 1 # IoU 손실 가중치 (1)
        loss_class: 1 # obj_score 손실 가중치 (1)
      supervise_all_iou: true # 3개 마스크 모두에 대해 IoU 손실 적용 여부
      iou_use_l1_loss: true # Score loss에 L1 손실 사용 여부
      pred_obj_scores: true # 객체 점수 예측 사용 여부 (기본: true, 이미지: false로 변경 필요)
      focal_gamma_obj_score: 0.0 # Focal Loss 감마 값 (기본: 0.0 -> 이 경우 BCE와 동일)
      focal_alpha_obj_score: -1.0 # Focal Loss 알파 값 (기본: -1.0 -> 이 경우 클래스 불균형에 대한 조정 없음)

  distributed: # 분산 학습 설정 (멀티 GPU 사용 시)
    backend: nccl # 통신 규약
    find_unused_parameters: True # 학습 안정성 관련 옵션 (기본: True)

  logging: # 로깅 설정
    tensorboard_writer: # 텐서보드 설정 (학습 중 변화 정도 확인)
      _target_: training.utils.logger.make_tensorboard_logger 
      log_dir:  ${launcher.experiment_log_dir}/tensorboard # 로그 디렉토리 경로 (lanchurer에서 디렉토리 정의)
      flush_secs: 120
      should_log: True
    log_dir: ${launcher.experiment_log_dir}/logs # 로그 디렉토리 경로 (lanchurer에서 디렉토리 정의)
    log_freq: 10 # 10 스텝마다 로그 기록

  # initialize from a SAM 2 checkpoint
  checkpoint: # 체크포인트 로딩
    save_dir: ${launcher.experiment_log_dir}/checkpoints # 체크포인트 저장 디렉토리 경로 (lanchurer에서 디렉토리 정의)
    save_freq: 0 # 저장 빈도 (0: 마지막 에폭만 저장, 1: 1에폭마다 저장, 2: 2에폭마다 저장, ...)
    model_weight_initializer: # 사전 학습된 가중치 로딩
      _partial_: True
      _target_: training.utils.checkpoint_utils.load_state_dict_into_model
      strict: True
      ignore_unexpected_keys: null
      ignore_missing_keys: null

      state_dict:
        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels
        checkpoint_path: ./checkpoints/sam2.1_hiera_base_plus.pt # 초기 가중치 파일 경로
        ckpt_state_dict_keys: ['model']

launcher:
  num_nodes: 1 # 1대의 서버 사용
  gpus_per_node: 8 # 8개의 gpu 사용 (로컬, gpu 1대밖에 없으므로 1로 변경)
  experiment_log_dir: null # 학습 결과를 저장할 디렉토리 경로 (반드시 지정)

# SLURM args if running on a cluster
submitit: # SLURM(클러ㅅ스터 시스템) 관련 설정
  partition: null
  account: null
  qos: null
  cpus_per_task: 10
  use_cluster: false # 로컬에서 실행하므로 false
  timeout_hour: 24
  name: null
  port_range: [10000, 65000]

